{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPgKn2is4JuX"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import layers, models, callbacks, optimizers\n",
        "from keras.utils import load_img, img_to_array\n",
        "from keras.optimizers import RMSprop, SGD, Adam\n",
        "from keras.applications import ResNet50, VGG16\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, AveragePooling2D, GlobalAveragePooling2D\n",
        "import re, sys, datetime, json, random, cv2, os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import os\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## If you are using the data by mounting the google drive, use the following :\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "metadata": {
        "id": "ogJNxGT55mcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/gdrive/MyDrive/images/face_recognition\""
      ],
      "metadata": {
        "id": "P398nbPF5rcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the directory for the training images\n",
        "test_dir = '/content/gdrive/MyDrive/images/face_recognition/test'"
      ],
      "metadata": {
        "id": "KxjJPrDA59qR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "names = {0: \"Dhiraj\", 1: \"Om\", 2: \"Tanushree\"}"
      ],
      "metadata": {
        "id": "wJQ9dSQs5-gD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_CLASSES = len(names)\n",
        "print(NUM_CLASSES)"
      ],
      "metadata": {
        "id": "zknio6oylaVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = \"/content/gdrive/MyDrive/images/face_recognition/train\"\n",
        "\n",
        "\"\"\"\n",
        "    # Use `ImageDataGenerator` to rescale the images.\n",
        "    # Create the train generator and specify where the train dataset directory, image size, batch size.\n",
        "    # Create the validation generator with similar approach as the train generator with the flow_from_directory() method.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "wVtBryfO4QH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We are trying to minimize the resolution of the images without loosing the 'Features'\n",
        "# For facial recognition, this seems to be working fine, you can increase or decrease it\n",
        "\n",
        "# Depending upon the total number of images you have set the batch size\n",
        "FAST_RUN = False\n",
        "IMAGE_WIDTH=224\n",
        "IMAGE_HEIGHT=224\n",
        "IMAGE_SIZE=(IMAGE_WIDTH, IMAGE_HEIGHT)\n",
        "IMAGE_CHANNELS=3\n",
        "BATCH_SIZE = 32\n",
        "IMAGE_SHAPE = (IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS)\n",
        "learning_rate = 0.0001\n",
        "target_loss = 0.8\n"
      ],
      "metadata": {
        "id": "wy_gwi3q4TVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop through all image files in the directory\n",
        "def resize_images(path_to_directory, target_size):\n",
        "  for filename in os.listdir(path_to_directory):\n",
        "        root, ext = os.path.splitext(filename)\n",
        "        if ext.lower() in [\".jpg\", \".jpeg\", \".png\"]:\n",
        "            img_path = os.path.join(path_to_directory, filename)\n",
        "            if Image.open(img_path).size != target_size:\n",
        "              img = Image.open(img_path).resize(target_size)\n",
        "              img.save(img_path)\n",
        "  print(f\"All images in {path_to_directory} resized to {target_size}\")"
      ],
      "metadata": {
        "id": "3O2dXIscPAT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, 3):\n",
        "  resize_images(os.path.join(train_dir, str(i)), IMAGE_SIZE)\n"
      ],
      "metadata": {
        "id": "WLUpQkrmPA4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, 3):\n",
        "  resize_images(os.path.join(test_dir, str(i)), IMAGE_SIZE)"
      ],
      "metadata": {
        "id": "4k1inFXMPOTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We need a data generator which rescales the images\n",
        "# Pre-processes the images like re-scaling and other required operations for the next steps\n",
        "test_datagen = ImageDataGenerator(\n",
        "    rescale=1. / 255\n",
        "    )\n",
        "\n",
        "# Create an instance of ImageDataGenerator for the training set\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,      # Rescale the pixel values to the range [0, 1]\n",
        "    rotation_range=20,   # Randomly rotate the images by up to 20 degrees\n",
        "    width_shift_range=0.2,  # Randomly shift the images horizontally by up to 20% of the image width\n",
        "    height_shift_range=0.2, # Randomly shift the images vertically by up to 20% of the image height\n",
        "    shear_range=0.2,         # Randomly shear the images by up to 20%\n",
        "    zoom_range=0.3,          # Randomly zoom the images by up to 20%\n",
        "    horizontal_flip=True,    # Randomly flip the images horizontally\n",
        "    vertical_flip=False,\n",
        "    brightness_range = (0.8, 1.2), # brightness adjuster so that some partial images can detect when ground is dark\n",
        "    fill_mode='nearest',      # Fill any empty pixels with the nearest available pixel value\n",
        "    validation_split=0.3\n",
        ")\n",
        "\n",
        "# Normalize the data\n",
        "train_datagen.mean = [123.68, 116.779, 103.939] # RGB channel mean values from ImageNet\n",
        "train_datagen.std = [58.393, 57.12, 57.375]\n",
        "\n",
        "# We separate out data set into Training, Validation & Testing. Mostly you will see Training and Validation.\n",
        "# We create generators for that, here we have train and validation generator.\n",
        "# Create a train_generator\n",
        "# Use the flow_from_directory method to load the images from the directory\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,                 # Path to the training set directory\n",
        "    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),    # Resize the images to 256x256\n",
        "    batch_size=BATCH_SIZE,             # Use batches of 32 images\n",
        "    class_mode='categorical',   # Use categorical cross-entropy loss\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "# Load the validation data into x_val and y_val\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    directory=train_dir,\n",
        "    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# Load the test data into x_test and y_test\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    directory=test_dir,\n",
        "    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n",
        "    batch_size=BATCH_SIZE//2,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Handle any exceptions that may occur during data generation\n",
        "try:\n",
        "    x_train, y_train = next(train_generator)\n",
        "except Exception as e:\n",
        "    print(f\"Error during training data generation: {e}\")\n",
        "    \n",
        "try:\n",
        "    x_val, y_val = next(validation_generator)\n",
        "except Exception as e:\n",
        "    print(f\"Error during validation data generation: {e}\")\n",
        "\n",
        "try:\n",
        "    x_test, y_test = next(test_generator)\n",
        "except Exception as e:\n",
        "    print(f\"Error during test data generation: {e}\")\n",
        "\n",
        "test_generator.class_indices"
      ],
      "metadata": {
        "id": "Y80Bljxx_zj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Triggering a training generator for all the batches\n",
        "for image_batch, label_batch in train_generator:\n",
        "    break\n",
        "\n",
        "# This will print all classification labels in the console\n",
        "print(train_generator.class_indices)\n",
        "\n",
        "# Creating a file which will contain all names in the format of next lines\n",
        "labels = '\\n'.join(sorted(train_generator.class_indices.keys()))\n",
        "\n",
        "# Writing it out to the file which will be named 'labels.txt'\n",
        "with open('labels.txt', 'w') as f:\n",
        "    f.write(labels)\n",
        "\n",
        "shutil.copy('labels.txt','gdrive/MyDrive/models/face_recognition_labels.txt')\n"
      ],
      "metadata": {
        "id": "W2Fi8rJ9_4lJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir_0 = os.path.join(train_dir + '/0')\n",
        "\n",
        "dir_1 = os.path.join(train_dir + '/1')\n",
        "\n",
        "dir_2 = os.path.join(train_dir + '/2')"
      ],
      "metadata": {
        "id": "Q1I5CR9nB5Mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "names_0 = random.sample(os.listdir(dir_0), 10)\n",
        "print(names_0[:10])\n",
        "\n",
        "names_1 = random.sample(os.listdir(dir_1), 10)\n",
        "print(names_1[:10])\n",
        "\n",
        "names_2 = random.sample(os.listdir(dir_2), 10)\n",
        "print(names_2[:10])\n"
      ],
      "metadata": {
        "id": "j-eiXmmpCKFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnt0 = len(os.listdir(dir_0))\n",
        "cnt1 = len(os.listdir(dir_1))\n",
        "cnt2 = len(os.listdir(dir_2))\n",
        "\n",
        "print('total training 0 images:', cnt0)\n",
        "print('total training 1 images:', cnt1)\n",
        "print('total training 2 images', cnt2)"
      ],
      "metadata": {
        "id": "UhsMeyJbCd_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "# Parameters for our graph; we'll output images in a 3x3 configuration\n",
        "nrows = 3\n",
        "ncols = 3\n",
        "\n",
        "# Index for iterating over images\n",
        "pic_index = 0"
      ],
      "metadata": {
        "id": "rKXKdIBYETiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up matplotlib fig, and size it to fit 4x4 pics\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(ncols * 3, nrows * 3)\n",
        "\n",
        "pic_index += 3\n",
        "pix0 = [os.path.join(dir_0, fname) \n",
        "                for fname in names_0[pic_index-3:pic_index]]\n",
        "pix1 = [os.path.join(dir_1, fname) \n",
        "                for fname in names_1[pic_index-3:pic_index]]\n",
        "pix2 = [os.path.join(dir_2, fname) \n",
        "                for fname in names_2[pic_index-3:pic_index]]\n",
        "\n",
        "for i, img_path in enumerate(pix0 + pix1 + pix2):\n",
        "  # Set up subplot; subplot indices start at 1\n",
        "  sp = plt.subplot(nrows, ncols, i + 1)\n",
        "  sp.axis('Off') # Don't show axes (or gridlines)\n",
        "\n",
        "  img = mpimg.imread(img_path)\n",
        "  plt.imshow(img)\n",
        "  if i < 3:\n",
        "    plt.text(2, 10, names[0], fontsize=15, color='red')\n",
        "  elif i >=3 and i < 6:\n",
        "    plt.text(2, 10, names[1], fontsize=15, color='red')\n",
        "  else:\n",
        "    plt.text(2, 10, names[2], fontsize=15, color='red')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ygtfjl5cC1Ue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"\n",
        "We have to create the base model from the pre-trained CNN\n",
        "\n",
        "Create the base model from the **MobileNet V2** model developed at Google\n",
        "and pre-trained on the ImageNet dataset, a large dataset of 1.4M images and 1000 classes of web images.\n",
        "\n",
        "First, pick which intermediate layer of MobileNet V2 will be used for feature extraction. \n",
        "A common practice is to use the output of the very last layer before the flatten operation,\n",
        "The so-called \"bottleneck layer\". The reasoning here is that the following fully-connected layers \n",
        "will be too specialized to the task the network was trained on, and thus the features learned by\n",
        "These layers won't be very useful for a new task. The bottleneck features, however, retain much generality.\n",
        "\n",
        "Let's instantiate an MobileNet V2 model pre-loaded with weights trained on ImageNet. \n",
        "By specifying the `include_top=False` argument, we load a network that doesn't include the\n",
        "classification layers at the top, which is ideal for feature extraction.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "72eBAjIbRbqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the base model from the pre-trained model MobileNet V2 Hidden and output layer is in the model The first\n",
        "# layer does a redundant work of classification of features which is not required to be trained\n",
        "# (This is also called as bottle neck layer)\n",
        "\n",
        "# Hence, creating a model with EXCLUDING the top layer\n",
        "base_model = tf.keras.applications.MobileNetV2(input_shape=IMAGE_SHAPE,\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')\n",
        "# base_model = VGG16(weights='imagenet', include_top=False, input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS))\n",
        "\n",
        "# Feature extraction\n",
        "# You will freeze the convolution base created from the previous step and use that as a feature extractor\n",
        "# Add a classifier on top of it and train the top-level classifier.\n",
        "\n",
        "# We will be tweaking the model with our own classifications\n",
        "# Hence we don't want our tweaks to affect the layers in the 'base_model'\n",
        "# Hence we disable their training\n",
        "# Freeze all layers except the last one\n",
        "# for layer in base_model.layers[: -2]:\n",
        "#     layer.trainable = False\n",
        "base_model.trainable = False\n",
        "\n"
      ],
      "metadata": {
        "id": "J89Tq8XM4Zlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# very normal custom model based on the base model\n",
        "model = models.Sequential()\n",
        "model.add(base_model)\n",
        "model.add(layers.AveragePooling2D(pool_size=(2, 2)))\n",
        "model.add(layers.Conv2D(filters=32, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"))\n",
        "model.add(layers.MaxPooling2D(pool_size=(3,3), strides=(2,2), padding=\"same\"))\n",
        "model.add(layers.Dropout(0.25))\n",
        "\n",
        "model.add(layers.Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"))\n",
        "model.add(layers.MaxPooling2D(pool_size=(3,3), strides=(2,2), padding=\"same\"))\n",
        "model.add(layers.Dropout(0.25))\n",
        "\n",
        "model.add(layers.Conv2D(filters=128, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"))\n",
        "model.add(layers.Dropout(0.25))\n",
        "model.add(layers.GlobalAveragePooling2D())\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(NUM_CLASSES, activation='softmax'))\n"
      ],
      "metadata": {
        "id": "IkPKQfDN4o1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You must compile the model before training it.  Since there are two classes, use a binary cross-entropy loss.\n",
        "# Since we have added more classification nodes, to our existing model, we need to compile the whole thing\n",
        "# as a single model, hence we will compile the model now\n",
        "\n",
        "# 1 - BP optimizer [Adam/Xavier algorithms help in Optimization]\n",
        "# 2 - Weights are changed depending upon the 'LOSS' ['RMS, 'CROSS-ENTROPY' are some algorithms]\n",
        "# 3 - On basis of which parameter our loss will be calculated? here we are going for accuracy\n",
        "optimizer = Adam(lr=learning_rate)\n",
        "\n",
        "model.compile(optimizer=optimizer, \n",
        "              loss='categorical_crossentropy', \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# model.compile(loss='categorical_crossentropy',\n",
        "#               optimizer=RMSprop(learning_rate=0.0001),\n",
        "#               metrics=['accuracy'])\n",
        "\n",
        "# To see the model summary in a tabular structure\n",
        "model.summary()\n",
        "\n",
        "# Printing some statistics\n",
        "print('Number of trainable variables = {}'.format(len(model.trainable_variables)))\n",
        "\n"
      ],
      "metadata": {
        "id": "Tr0JaDQ-4ts2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# callback class\n",
        "class myCallback(callbacks.Callback):\n",
        "    def __init__(self, target_loss):\n",
        "        super(myCallback, self).__init__()\n",
        "        self.target_loss = target_loss\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        '''\n",
        "        Halts the training after reaching 98 percent accuracy\n",
        "\n",
        "        Args:\n",
        "            epoch (integer) - index of epoch (required but unused in the function definition below)\n",
        "            logs (dict) - metric results from the training epoch\n",
        "        '''\n",
        "\n",
        "        # Check accuracy\n",
        "        if(logs.get('accuracy') > 0.98 and logs.get('val_accuracy') > 0.83 and logs.get('val_loss') < 0.40):\n",
        "\n",
        "            # Stop if threshold is met\n",
        "            print(\"\\nTraining and validation accuracy are higher than 0.98 so cancelling training!\")\n",
        "            self.model.stop_training = True\n",
        "\n",
        "# Instantiate callbacks\n",
        "callbacks = [\n",
        "    myCallback(target_loss=target_loss),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(factor=0.1, patience=3)\n",
        "]\n"
      ],
      "metadata": {
        "id": "XBvo0Q-kUeOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "# We will do it in 50 Iterations\n",
        "epochs = 50\n",
        "\n",
        "# Fitting / Training the model\n",
        "history = model.fit(train_generator,\n",
        "                    epochs=epochs,\n",
        "                    validation_data=validation_generator,\n",
        "                    callbacks=[callbacks])\n",
        "\n"
      ],
      "metadata": {
        "id": "0WWiSqH54xIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the Learning curves [OPTIONAL]\n",
        "#\n",
        "# Let's take a look at the learning curves of the training and validation accuracy/loss\n",
        "# When using the MobileNet V2 base model as a fixed feature extractor.\n",
        "\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([min(plt.ylim()), 1])\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.ylim([0, 1.0])\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\"\"\"## Fine tuning\n",
        "In our feature extraction experiment, you were only training a few layers on top of an MobileNet V2 base model. The weights of the pre-trained network were **not** updated during training.\n",
        "\n",
        "One way to increase performance even further is to train (or \"fine-tune\") the weights of the top layers of the pre-trained model alongside the training of the classifier you added. The training process will force the weights to be tuned from generic features maps to features associated specifically to our dataset.\n",
        "\n",
        "### Un-freeze the top layers of the model\n",
        "\n",
        "All you need to do is unfreeze the `base_model` and set the bottom layers be un-trainable. Then, recompile the model (necessary for these changes to take effect), and resume training.\n",
        "\"\"\"\n",
        "\n",
        "# Set the base model to trainable\n",
        "base_model.trainable = True\n",
        "\n",
        "# Print the layer names in the base model\n",
        "print(\"Base model layers:\")\n",
        "for layer in base_model.layers:\n",
        "    print(layer.name)\n",
        "\n",
        "# Fine-tune from this layer onwards\n",
        "fine_tune_from_layer = -2\n",
        "\n",
        "# Freeze all the layers before the `fine_tune_from_layer` layer\n",
        "for layer in base_model.layers[:fine_tune_from_layer]:\n",
        "    layer.trainable = False\n"
      ],
      "metadata": {
        "id": "J-ixfxTY4zce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "# Compile the model using a much lower training rate.\n",
        "# Notice the parameter in Adam() function, parameter passed to Adam is the learning rate\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-5),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Getting the summary of the final model\n",
        "model.summary()\n",
        "# Printing Training Variables\n",
        "print('Number of trainable variables = {}'.format(len(model.trainable_variables)))\n",
        "\n"
      ],
      "metadata": {
        "id": "xDu-csvT45dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Continue Train the model\n",
        "history_fine = model.fit(train_generator,\n",
        "                         epochs=5,\n",
        "                         validation_data=validation_generator\n",
        "                         )\n",
        "# Saving the Trained Model to the keras h5 format.\n",
        "# So in future, if we want to convert again, we don't have to go through the whole process again\n",
        "saved_model_dir = 'gdrive/MyDrive/models/face_recognition.h5'\n",
        "model.save(saved_model_dir)\n",
        "print(\"Model Saved to gdrive/MyDrive/models/face_recognition.h5\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "efXhcdpb472p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model\n",
        "loaded_model = models.load_model('gdrive/MyDrive/models/face_recognition.h5')"
      ],
      "metadata": {
        "id": "aa5ojNHKmsZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from keras.preprocessing.image import load_img, img_to_array\n",
        "from keras.utils import load_img, img_to_array\n",
        "import numpy as np\n",
        "\n",
        "# img_name = \"1/Om.jpg\"\n",
        "# img_name = \"0/Dhirajx.jpg\"\n",
        "# img_name = \"2/Tanushree.jpg\"\n",
        "# img_name = \"2/Tanushree2.jpg\"\n",
        "img_name = \"2/Tanushree3.jpg\"\n",
        "# img_name = \"2/tanushree_passport_sign.jpg\"\n",
        "img_path = os.path.join(test_dir, img_name)\n",
        "\n",
        "# Load and preprocess your image\n",
        "img = load_img(img_path, target_size=(224, 224))\n",
        "# img = load_img(img_path, target_size=(128, 128))\n",
        "# Show image\n",
        "plt.imshow(img)\n",
        "plt.show()\n",
        "\n",
        "img = img_to_array(img)\n",
        "img = np.expand_dims(img, axis=0)\n",
        "img = img/255.0\n",
        "\n",
        "# Make a prediction\n",
        "prediction = loaded_model.predict(img)\n",
        "print(prediction)\n",
        "# Get the predicted class\n",
        "predicted_class = np.argmax(prediction, axis=1)\n",
        "\n",
        "print(predicted_class)\n",
        "print(names[predicted_class[0]])"
      ],
      "metadata": {
        "id": "x47uH3aTnQK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Convert the model\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()"
      ],
      "metadata": {
        "id": "uMfIEzoQoAAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model.\n",
        "with open('face_recognition_model.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)\n",
        "\n",
        "shutil.copy('face_recognition_model.tflite','gdrive/MyDrive/models/face_recognition_model.tflite')"
      ],
      "metadata": {
        "id": "PrpGYsI1oBE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # tf.saved_model.save(model, saved_model_dir)\n",
        "\n",
        "# converter = tf.lite.TFLiteConverter.from_keras_model_file(model)\n",
        "# # Use this if 238 fails tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "\n",
        "# tflite_model = converter.convert()\n",
        "\n",
        "# with open('model.tflite', 'wb') as f:\n",
        "#     f.write(tflite_model)\n",
        "\n",
        "# Let's take a look at the learning curves of the training and validation accuracy/loss\n",
        "# When fine tuning the last few layers of the MobileNet V2 base model and training the classifier on top of it.\n",
        "# The validation loss is much higher than the training loss, so you may get some overfitting.\n",
        "# You may also get some overfitting as the new training set is\n",
        "# Relatively small and similar to the original MobileNet V2 datasets.\n",
        "\n",
        "acc = history_fine.history['accuracy']\n",
        "val_acc = history_fine.history['val_accuracy']\n",
        "print(acc)\n",
        "print(val_acc)\n",
        "loss = history_fine.history['loss']\n",
        "val_loss = history_fine.history['val_loss']\n",
        "print(loss)\n",
        "print(val_loss)\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([min(plt.ylim()), 1])\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.ylim([0, 1.0])\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "J2DJ6u6o498B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary:\n",
        "'''\n",
        "* **Using a pre-trained model for feature extraction**:  When working with a small dataset, it is common to take advantage of features learned by a model trained on a larger dataset in the same domain. This is done by instantiating the pre-trained model and adding a fully-connected classifier on top. The pre-trained model is \"frozen\" and only the weights of the classifier get updated during training.\n",
        "In this case, the convolutional base extracted all the features associated with each image and you just trained a classifier that determines the image class given that set of extracted features.\n",
        "\n",
        "* **Fine-tuning a pre-trained model**: To further improve performance, one might want to repurpose the top-level layers of the pre-trained models to the new dataset via fine-tuning.\n",
        "In this case, you tuned your weights such that your model learned high-level features specific to the dataset. This technique is usually recommended when the training dataset is large and very similar to the orginial dataset that the pre-trained model was trained on.\n",
        "'''"
      ],
      "metadata": {
        "id": "Xh2-alkNoWVb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "However this tflite will not run in edgetpu eg. raspberrypi\n",
        "So we need to do as following"
      ],
      "metadata": {
        "id": "TSeaqnbooeb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A generator that provides a representative dataset\n",
        "\n",
        "def representative_data_gen():\n",
        "  dataset_list = tf.data.Dataset.list_files(test_dir + '/*/*')\n",
        "  for i in range(100):\n",
        "    image = next(iter(dataset_list))\n",
        "    # file_type = os.path.splitext(image)[1]\n",
        "    # if file_type not in ['.jpeg', '.jpg', '.png', '.bmp']:\n",
        "    #   continue\n",
        "    try:\n",
        "      image = tf.io.read_file(image)\n",
        "      image = tf.io.decode_jpeg(image, channels=3)\n",
        "      image = tf.image.resize(image, [IMAGE_WIDTH, IMAGE_HEIGHT])\n",
        "      image = tf.cast(image / 255., tf.float32)\n",
        "      image = tf.expand_dims(image, 0)\n",
        "    except tf.errors.InvalidArgumentError as e:\n",
        "      continue\n",
        "    yield [image]\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "# This enables quantization\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "# This sets the representative dataset for quantization\n",
        "converter.representative_dataset = representative_data_gen\n",
        "# This ensures that if any ops can't be quantized, the converter throws an error\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "# For full integer quantization, though supported types defaults to int8 only, we explicitly declare it for clarity.\n",
        "converter.target_spec.supported_types = [tf.int8]\n",
        "# These set the input and output tensors to uint8 (added in r2.3)\n",
        "converter.inference_input_type = tf.uint8\n",
        "converter.inference_output_type = tf.uint8\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open('face_recognition_model_edge.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)\n",
        "\n",
        "shutil.copy('face_recognition_model_edge.tflite','gdrive/MyDrive/models/face_recognition_model_edge.tflite')\n"
      ],
      "metadata": {
        "id": "XfaBgc8BoXaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compile the edgetpu tflite model\n",
        "\n",
        "Finally, we're ready to compile the model for the Edge TPU.\n",
        "\n",
        "First download the Edge TPU Compiler:"
      ],
      "metadata": {
        "id": "QtKaxpGVo22L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
        "\n",
        "! echo \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list\n",
        "\n",
        "! sudo apt-get update\n",
        "\n",
        "! sudo apt-get install edgetpu-compiler\t"
      ],
      "metadata": {
        "id": "pEdc6lnNo3XS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! edgetpu_compiler face_recognition_model_edge.tflite\n",
        "\n",
        "shutil.copy('face_recognition_model_edge_edgetpu.tflite','gdrive/MyDrive/models/face_recognition_model_edge_edgetpu.tflite')"
      ],
      "metadata": {
        "id": "WlM2w_W5o5Gu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print (train_generator.class_indices)\n",
        "\n",
        "# labels = '\\n'.join(sorted(train_generator.class_indices.keys()))\n",
        "\n",
        "# with open('updated_snow_detection_on_solar_panel_labels.txt', 'w') as f:\n",
        "#   f.write(labels)\n",
        "# shutil.copy('updated_snow_detection_on_solar_panel_labels.txt','gdrive/MyDrive/models/updated_snow_detection_on_solar_panel_labels.txt')"
      ],
      "metadata": {
        "id": "Kebp4fcqpWGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('face_recognition_model.tflite')\n",
        "files.download('face_recognition_model_edge.tflite')\n",
        "files.download('face_recognition_model_edge_edgetpu.tflite')\n",
        "files.download('labels.txt')"
      ],
      "metadata": {
        "id": "4wbhXcSHpaU0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}